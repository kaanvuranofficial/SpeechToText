{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-25T03:11:28.510796Z",
     "iopub.status.busy": "2025-12-25T03:11:28.510550Z",
     "iopub.status.idle": "2025-12-25T03:11:37.266101Z",
     "shell.execute_reply": "2025-12-25T03:11:37.265177Z",
     "shell.execute_reply.started": "2025-12-25T03:11:28.510764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning==1.9.0 --quiet\n",
    "!pip install pandas --quiet\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:11:45.236917Z",
     "iopub.status.busy": "2025-12-25T03:11:45.236309Z",
     "iopub.status.idle": "2025-12-25T03:11:45.246861Z",
     "shell.execute_reply": "2025-12-25T03:11:45.246269Z",
     "shell.execute_reply.started": "2025-12-25T03:11:45.236860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Checking LibriSpeech dataset...\")\n",
    "librispeech_path = \"/kaggle/input/librispeech-asr-corpus/\"\n",
    "if os.path.exists(librispeech_path):\n",
    "    print(\"LibriSpeech dataset found\")\n",
    "    print(\"\\nAvailable splits:\")\n",
    "    for item in os.listdir(librispeech_path):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"LibriSpeech not found. Add it via 'Add Data' button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:11:58.150707Z",
     "iopub.status.busy": "2025-12-25T03:11:58.150432Z",
     "iopub.status.idle": "2025-12-25T03:11:58.155277Z",
     "shell.execute_reply": "2025-12-25T03:11:58.154584Z",
     "shell.execute_reply.started": "2025-12-25T03:11:58.150683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "class TextProcess:\n",
    "    \"\"\"Text processor for converting between text and integer sequences\"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_map = {}\n",
    "        self.int_map = {}\n",
    "        \n",
    "        chars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', \n",
    "                 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        \n",
    "        self.char_map[\"'\"] = 0\n",
    "        self.char_map[' '] = 1\n",
    "        for i, char in enumerate(chars):\n",
    "            self.char_map[char] = i + 2\n",
    "        self.char_map['_'] = 28\n",
    "        \n",
    "        self.int_map = {v: k for k, v in self.char_map.items()}\n",
    "    \n",
    "    def text_to_int_sequence(self, text):\n",
    "        text = text.lower()\n",
    "        return [self.char_map.get(c, 1) for c in text if c in self.char_map]\n",
    "    \n",
    "    def int_to_text_sequence(self, labels):\n",
    "        return ''.join([self.int_map.get(i, '') for i in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:12:06.476252Z",
     "iopub.status.busy": "2025-12-25T03:12:06.475524Z",
     "iopub.status.idle": "2025-12-25T03:12:06.482089Z",
     "shell.execute_reply": "2025-12-25T03:12:06.480749Z",
     "shell.execute_reply.started": "2025-12-25T03:12:06.476225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ActDropNormCNN1D(nn.Module):\n",
    "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
    "        super(ActDropNormCNN1D, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(n_feats)\n",
    "        self.keep_shape = keep_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(F.gelu(self.norm(x)))\n",
    "        if self.keep_shape:\n",
    "            return x.transpose(1, 2)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class SpeechRecognition(nn.Module):\n",
    "    hyper_parameters = {\n",
    "        \"num_classes\": 29,\n",
    "        \"n_feats\": 81,\n",
    "        \"dropout\": 0.1,\n",
    "        \"hidden_size\": 1024,\n",
    "        \"num_layers\": 1\n",
    "    }\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
    "        super(SpeechRecognition, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
    "            ActDropNormCNN1D(n_feats, dropout),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(n_feats, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, dropout=0.0,\n",
    "                            bidirectional=False)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.final_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        n, hs = self.num_layers, self.hidden_size\n",
    "        return (torch.zeros(n*1, batch_size, hs),\n",
    "                torch.zeros(n*1, batch_size, hs))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.squeeze(1)\n",
    "        x = self.cnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        out, (hn, cn) = self.lstm(x, hidden)\n",
    "        x = self.dropout2(F.gelu(self.layer_norm2(out)))\n",
    "        return self.final_fc(x), (hn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:12:17.642468Z",
     "iopub.status.busy": "2025-12-25T03:12:17.642151Z",
     "iopub.status.idle": "2025-12-25T03:12:17.648933Z",
     "shell.execute_reply": "2025-12-25T03:12:17.648294Z",
     "shell.execute_reply.started": "2025-12-25T03:12:17.642442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import TextProcess\n",
    "\n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self, rate, policy=3, freq_mask=15, time_mask=35):\n",
    "        super(SpecAugment, self).__init__()\n",
    "        self.rate = rate\n",
    "        self.specaug = nn.Sequential(\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
    "        )\n",
    "        self.specaug2 = nn.Sequential(\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask),\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
    "        )\n",
    "        policies = {1: self.policy1, 2: self.policy2, 3: self.policy3}\n",
    "        self._forward = policies[policy]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._forward(x)\n",
    "    \n",
    "    def policy1(self, x):\n",
    "        probability = torch.rand(1, 1).item()\n",
    "        if self.rate > probability:\n",
    "            return self.specaug(x)\n",
    "        return x\n",
    "    \n",
    "    def policy2(self, x):\n",
    "        probability = torch.rand(1, 1).item()\n",
    "        if self.rate > probability:\n",
    "            return self.specaug2(x)\n",
    "        return x\n",
    "    \n",
    "    def policy3(self, x):\n",
    "        probability = torch.rand(1, 1).item()\n",
    "        if probability > 0.5:\n",
    "            return self.policy1(x)\n",
    "        return self.policy2(x)\n",
    "\n",
    "class LogMelSpec(nn.Module):\n",
    "    def __init__(self, sample_rate=8000, n_mels=128, win_length=160, hop_length=80):\n",
    "        super(LogMelSpec, self).__init__()\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_mels=n_mels,\n",
    "            win_length=win_length, hop_length=hop_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = np.log(x + 1e-14)\n",
    "        return x\n",
    "\n",
    "def get_featurizer(sample_rate, n_feats=81):\n",
    "    return LogMelSpec(sample_rate=sample_rate, n_mels=n_feats, win_length=160, hop_length=80)\n",
    "\n",
    "class Data(torch.utils.data.Dataset):\n",
    "    parameters = {\n",
    "        \"sample_rate\": 8000, \"n_feats\": 81,\n",
    "        \"specaug_rate\": 0.5, \"specaug_policy\": 3,\n",
    "        \"time_mask\": 70, \"freq_mask\": 15\n",
    "    }\n",
    "    \n",
    "    def __init__(self, json_path, sample_rate, n_feats, specaug_rate, specaug_policy,\n",
    "                 time_mask, freq_mask, valid=False, shuffle=True, text_to_int=True, log_ex=True):\n",
    "        self.log_ex = log_ex\n",
    "        self.text_process = TextProcess()\n",
    "        \n",
    "        print(\"Loading data json file from\", json_path)\n",
    "        self.data = pd.read_json(json_path, lines=True)\n",
    "        \n",
    "        if valid:\n",
    "            self.audio_transforms = torch.nn.Sequential(\n",
    "                LogMelSpec(sample_rate=sample_rate, n_mels=n_feats, win_length=160, hop_length=80)\n",
    "            )\n",
    "        else:\n",
    "            self.audio_transforms = torch.nn.Sequential(\n",
    "                LogMelSpec(sample_rate=sample_rate, n_mels=n_feats, win_length=160, hop_length=80),\n",
    "                SpecAugment(specaug_rate, specaug_policy, freq_mask, time_mask)\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.item()\n",
    "        \n",
    "        try:\n",
    "            file_path = self.data.key.iloc[idx]\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            \n",
    "            # Resample to 8000 Hz if needed\n",
    "            if sample_rate != 8000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 8000)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            label = self.text_process.text_to_int_sequence(self.data['text'].iloc[idx])\n",
    "            spectrogram = self.audio_transforms(waveform)\n",
    "            spec_len = spectrogram.shape[-1] // 2\n",
    "            label_len = len(label)\n",
    "            \n",
    "            if spec_len < label_len:\n",
    "                raise Exception('spectrogram len is bigger then label len')\n",
    "            if spectrogram.shape[0] > 1:\n",
    "                raise Exception('dual channel, skipping audio file %s' % file_path)\n",
    "            if spectrogram.shape[2] > 1650:\n",
    "                raise Exception('spectrogram to big. size %s' % spectrogram.shape[2])\n",
    "            if label_len == 0:\n",
    "                raise Exception('label len is zero... skipping %s' % file_path)\n",
    "        except Exception as e:\n",
    "            if self.log_ex:\n",
    "                print(str(e), file_path)\n",
    "            return self.__getitem__(idx - 1 if idx != 0 else idx + 1)\n",
    "        return spectrogram, label, spec_len, label_len\n",
    "    \n",
    "    def describe(self):\n",
    "        return self.data.describe()\n",
    "\n",
    "def collate_fn_padd(data):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (spectrogram, label, input_length, label_length) in data:\n",
    "        if spectrogram is None:\n",
    "            continue\n",
    "        spectrograms.append(spectrogram.squeeze(0).transpose(0, 1))\n",
    "        labels.append(torch.Tensor(label))\n",
    "        input_lengths.append(input_length)\n",
    "        label_lengths.append(label_length)\n",
    "    \n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:37:54.345446Z",
     "iopub.status.busy": "2025-12-25T03:37:54.344465Z",
     "iopub.status.idle": "2025-12-25T03:37:54.353101Z",
     "shell.execute_reply": "2025-12-25T03:37:54.352377Z",
     "shell.execute_reply.started": "2025-12-25T03:37:54.345400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import ArgumentParser\n",
    "from model import SpeechRecognition\n",
    "from dataset import Data, collate_fn_padd\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "class SpeechModule(LightningModule):\n",
    "    def __init__(self, model, args):\n",
    "        super(SpeechModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        return self.model(x, hidden)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), self.args.learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min',\n",
    "            factor=0.50, patience=6)\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "    \n",
    "    def step(self, batch):\n",
    "        spectrograms, labels, input_lengths, label_lengths = batch\n",
    "        bs = spectrograms.shape[0]\n",
    "        hidden = self.model._init_hidden(bs)\n",
    "        hn, c0 = hidden[0].to(self.device), hidden[1].to(self.device)\n",
    "        output, _ = self(spectrograms, (hn, c0))\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        loss = self.criterion(output, labels, input_lengths, label_lengths)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('lr', self.optimizer.param_groups[0]['lr'])\n",
    "        return loss\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        d_params = Data.parameters\n",
    "        d_params.update(self.args.dparams_override)\n",
    "        train_dataset = Data(json_path=self.args.train_file, **d_params)\n",
    "        return DataLoader(dataset=train_dataset,\n",
    "                          batch_size=self.args.batch_size,\n",
    "                          num_workers=self.args.data_workers,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=collate_fn_padd)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        d_params = Data.parameters\n",
    "        d_params.update(self.args.dparams_override)\n",
    "        test_dataset = Data(json_path=self.args.valid_file, **d_params, valid=True)\n",
    "        return DataLoader(dataset=test_dataset,\n",
    "                          batch_size=self.args.batch_size,\n",
    "                          num_workers=self.args.data_workers,\n",
    "                          collate_fn=collate_fn_padd,\n",
    "                          pin_memory=True)\n",
    "\n",
    "def checkpoint_callback(args):\n",
    "    return ModelCheckpoint(\n",
    "        dirpath=args.save_model_path,\n",
    "        save_top_k=3,\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        filename='speech-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "\n",
    "def main(args):\n",
    "    h_params = SpeechRecognition.hyper_parameters\n",
    "    h_params.update(args.hparams_override)\n",
    "    model = SpeechRecognition(**h_params)\n",
    "    \n",
    "    if args.load_model_from:\n",
    "        speech_module = SpeechModule.load_from_checkpoint(args.load_model_from, model=model, args=args)\n",
    "    else:\n",
    "        speech_module = SpeechModule(model, args)\n",
    "    \n",
    "    logger = TensorBoardLogger(args.logdir, name='speech_recognition')\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        accelerator='gpu',\n",
    "        devices=args.gpus,\n",
    "        logger=logger,\n",
    "        gradient_clip_val=1.0,\n",
    "        val_check_interval=args.valid_every,\n",
    "        callbacks=[checkpoint_callback(args)]\n",
    "    )\n",
    "    trainer.fit(speech_module)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('-g', '--gpus', default=1, type=int)\n",
    "    parser.add_argument('-w', '--data_workers', default=2, type=int)\n",
    "    parser.add_argument('--train_file', required=True, type=str)\n",
    "    parser.add_argument('--valid_file', required=True, type=str)\n",
    "    parser.add_argument('--valid_every', default=1000, type=int)\n",
    "    parser.add_argument('--save_model_path', required=True, type=str)\n",
    "    parser.add_argument('--load_model_from', default=None, type=str)\n",
    "    parser.add_argument('--logdir', default='tb_logs', type=str)\n",
    "    parser.add_argument('--epochs', default=10, type=int)\n",
    "    parser.add_argument('--batch_size', default=32, type=int)\n",
    "    parser.add_argument('--learning_rate', default=1e-3, type=float)\n",
    "    parser.add_argument(\"--hparams_override\", default=\"{}\", type=str)\n",
    "    parser.add_argument(\"--dparams_override\", default=\"{}\", type=str)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.hparams_override = ast.literal_eval(args.hparams_override)\n",
    "    args.dparams_override = ast.literal_eval(args.dparams_override)\n",
    "    \n",
    "    os.makedirs(args.save_model_path, exist_ok=True)\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:38:00.927952Z",
     "iopub.status.busy": "2025-12-25T03:38:00.927624Z",
     "iopub.status.idle": "2025-12-25T03:38:04.515665Z",
     "shell.execute_reply": "2025-12-25T03:38:04.514952Z",
     "shell.execute_reply.started": "2025-12-25T03:38:00.927921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_librispeech_transcript(trans_file):\n",
    "    \"\"\"Parse LibriSpeech transcript file\"\"\"\n",
    "    transcripts = {}\n",
    "    with open(trans_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                file_id, text = parts\n",
    "                transcripts[file_id] = text.lower()\n",
    "    return transcripts\n",
    "\n",
    "def find_audio_files_recursive(base_path, max_samples=None):\n",
    "    \"\"\"Recursively find all FLAC files and their transcripts\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    print(f\" Searching for audio files in: {base_path}\\n\")\n",
    "    \n",
    "    # Walk through all directories\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Look for transcript files\n",
    "        for file in files:\n",
    "            if file.endswith('.trans.txt'):\n",
    "                trans_file = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    transcripts = parse_librispeech_transcript(trans_file)\n",
    "                    \n",
    "                    # Find corresponding audio files in same directory\n",
    "                    for audio_file in os.listdir(root):\n",
    "                        if audio_file.endswith('.flac'):\n",
    "                            file_id = audio_file.replace('.flac', '')\n",
    "                            if file_id in transcripts:\n",
    "                                audio_path = os.path.join(root, audio_file)\n",
    "                                data.append({\n",
    "                                    \"key\": audio_path,\n",
    "                                    \"text\": transcripts[file_id]\n",
    "                                })\n",
    "                                \n",
    "                                # Print progress every 100 files\n",
    "                                if len(data) % 100 == 0:\n",
    "                                    print(f\"   Found {len(data)} audio files...\", end='\\r')\n",
    "                                \n",
    "                                if max_samples and len(data) >= max_samples:\n",
    "                                    print(f\"\\nâœ… Reached max samples limit ({max_samples})\")\n",
    "                                    return data\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error reading {trans_file}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ===========================================================================\n",
    "# MAIN EXECUTION\n",
    "# ===========================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" PREPARING LIBRISPEECH DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "base_path = \"/kaggle/input/librispeech-asr-corpus\"\n",
    "\n",
    "# Check if path exists\n",
    "if not os.path.exists(base_path):\n",
    "    print(f\" Path not found: {base_path}\")\n",
    "    print(\"\\n Available paths:\")\n",
    "    if os.path.exists(\"/kaggle/input/\"):\n",
    "        for item in os.listdir(\"/kaggle/input/\"):\n",
    "            print(f\" {item}\")\n",
    "else:\n",
    "    print(f\" Found dataset at: {base_path}\\n\")\n",
    "    \n",
    "    # Show directory structure\n",
    "    print(\" Directory structure:\")\n",
    "    for item in sorted(os.listdir(base_path))[:15]:\n",
    "        item_path = os.path.join(base_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  {item}/\")\n",
    "            # Show subdirectories\n",
    "            try:\n",
    "                sub_items = os.listdir(item_path)[:3]\n",
    "                for sub in sub_items:\n",
    "                    print(f\"      â””â”€ {sub}\")\n",
    "                if len(os.listdir(item_path)) > 3:\n",
    "                    print(f\"      â””â”€ ... and {len(os.listdir(item_path)) - 3} more\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"   ðŸ“„ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING TRAINING DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create training data\n",
    "    # This will search the entire dataset recursively\n",
    "    train_data = find_audio_files_recursive(\n",
    "        base_path,\n",
    "        max_samples=5000  # Limit to 5000 for quick testing\n",
    "    )\n",
    "    \n",
    "    if len(train_data) > 0:\n",
    "        # Split into train and validation (90% train, 10% valid)\n",
    "        split_idx = int(len(train_data) * 0.9)\n",
    "        \n",
    "        train_samples = train_data[:split_idx]\n",
    "        valid_samples = train_data[split_idx:]\n",
    "        \n",
    "        # Ensure we have at least some validation samples\n",
    "        if len(valid_samples) < 100 and len(train_samples) > 100:\n",
    "            valid_samples = train_samples[-100:]\n",
    "            train_samples = train_samples[:-100]\n",
    "        \n",
    "        print(f\"\\n Saving dataset files...\")\n",
    "        \n",
    "        # Save training data\n",
    "        with open(\"train_data.json\", \"w\") as f:\n",
    "            for item in train_samples:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "        # Save validation data\n",
    "        with open(\"valid_data.json\", \"w\") as f:\n",
    "            for item in valid_samples:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"DATASET SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\" Training samples: {len(train_samples)}\")\n",
    "        print(f\" Validation samples: {len(valid_samples)}\")\n",
    "        print(f\" Total samples: {len(train_data)}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\n Sample training data:\")\n",
    "        sample = train_samples[0]\n",
    "        print(f\"   Audio: {sample['key']}\")\n",
    "        print(f\"   Text: {sample['text'][:100]}...\" if len(sample['text']) > 100 else f\"   Text: {sample['text']}\")\n",
    "        \n",
    "        # Verify audio file\n",
    "        print(f\"\\n Verifying sample audio...\")\n",
    "        try:\n",
    "            import torchaudio\n",
    "            waveform, sample_rate = torchaudio.load(sample['key'])\n",
    "            print(f\"   Audio loaded successfully!\")\n",
    "            print(f\"   Sample rate: {sample_rate} Hz\")\n",
    "            print(f\"   Duration: {waveform.shape[1] / sample_rate:.2f} seconds\")\n",
    "            print(f\"   Channels: {waveform.shape[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error loading audio: {e}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n NO AUDIO FILES FOUND!\")\n",
    "        print(\"\\n Debugging information:\")\n",
    "        print(\"\\nSearching for .flac files...\")\n",
    "        flac_files = []\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.flac'):\n",
    "                    flac_files.append(os.path.join(root, file))\n",
    "                    if len(flac_files) >= 5:\n",
    "                        break\n",
    "            if len(flac_files) >= 5:\n",
    "                break\n",
    "        \n",
    "        if flac_files:\n",
    "            print(f\" Found {len(flac_files)} .flac files (showing first 5):\")\n",
    "            for f in flac_files[:5]:\n",
    "                print(f\"   {f}\")\n",
    "        else:\n",
    "            print(\" No .flac files found at all!\")\n",
    "        \n",
    "        print(\"\\nSearching for .trans.txt files...\")\n",
    "        trans_files = []\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.trans.txt'):\n",
    "                    trans_files.append(os.path.join(root, file))\n",
    "                    if len(trans_files) >= 3:\n",
    "                        break\n",
    "            if len(trans_files) >= 3:\n",
    "                break\n",
    "        \n",
    "        if trans_files:\n",
    "            print(f\" Found transcript files (showing first 3):\")\n",
    "            for f in trans_files[:3]:\n",
    "                print(f\"   {f}\")\n",
    "        else:\n",
    "            print(\" No .trans.txt files found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===========================================================================\n",
    "# TIPS FOR FULL DATASET\n",
    "# ===========================================================================\n",
    "\"\"\"\n",
    "To use the FULL LibriSpeech dataset (not just 5000 samples):\n",
    "\n",
    "Change line 66 to:\n",
    "    max_samples=None  # Use ALL samples\n",
    "\n",
    "This will take longer but give better accuracy:\n",
    "- Full train-clean-100: ~28,000 samples (2-3 hours to prepare)\n",
    "- Full train-clean-360: ~104,000 samples (8-10 hours to prepare)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:38:10.116630Z",
     "iopub.status.busy": "2025-12-25T03:38:10.116280Z",
     "iopub.status.idle": "2025-12-25T03:38:10.129714Z",
     "shell.execute_reply": "2025-12-25T03:38:10.128971Z",
     "shell.execute_reply.started": "2025-12-25T03:38:10.116603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"Checking sample data...\")\n",
    "with open(\"train_data.json\", 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "    print(\"\\n Sample training data:\")\n",
    "    print(f\"Audio: {sample['key']}\")\n",
    "    print(f\"Text: {sample['text']}\")\n",
    "\n",
    "# Verify audio file exists\n",
    "import torchaudio\n",
    "try:\n",
    "    waveform, sample_rate = torchaudio.load(sample['key'])\n",
    "    print(f\"\\n Audio loaded successfully!\")\n",
    "    print(f\"Sample rate: {sample_rate} Hz\")\n",
    "    print(f\"Duration: {waveform.shape[1] / sample_rate:.2f} seconds\")\n",
    "    print(f\"Channels: {waveform.shape[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading audio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:38:13.445089Z",
     "iopub.status.busy": "2025-12-25T03:38:13.444582Z",
     "iopub.status.idle": "2025-12-25T03:38:22.998537Z",
     "shell.execute_reply": "2025-12-25T03:38:22.997681Z",
     "shell.execute_reply.started": "2025-12-25T03:38:13.445055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"tb_logs\", exist_ok=True)\n",
    "\n",
    "print(\" Starting training...\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  â€¢ GPU: Enabled\")\n",
    "print(f\"  â€¢ Batch size: 16\")\n",
    "print(f\"  â€¢ Epochs: 10\")\n",
    "print(f\"  â€¢ Learning rate: 0.001\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Start training\n",
    "!python train.py \\\n",
    "    --train_file train_data.json \\\n",
    "    --valid_file valid_data.json \\\n",
    "    --save_model_path models/ \\\n",
    "    --batch_size 16 \\\n",
    "    --epochs 10 \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --gpus 1 \\\n",
    "    --data_workers 2 \\\n",
    "    --valid_every 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:39:53.121341Z",
     "iopub.status.busy": "2025-12-25T03:39:53.120570Z",
     "iopub.status.idle": "2025-12-25T03:39:53.130632Z",
     "shell.execute_reply": "2025-12-25T03:39:53.129835Z",
     "shell.execute_reply.started": "2025-12-25T03:39:53.121311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:40:01.439925Z",
     "iopub.status.busy": "2025-12-25T03:40:01.439343Z",
     "iopub.status.idle": "2025-12-25T03:40:01.447009Z",
     "shell.execute_reply": "2025-12-25T03:40:01.446391Z",
     "shell.execute_reply.started": "2025-12-25T03:40:01.439878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import SpeechRecognition\n",
    "import os\n",
    "\n",
    "print(\"Finding best model checkpoint...\")\n",
    "checkpoints = [f for f in os.listdir(\"models/\") if f.endswith('.ckpt')]\n",
    "if checkpoints:\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: os.path.getctime(os.path.join(\"models/\", x)))\n",
    "    print(f\" Found checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    checkpoint_path = os.path.join(\"models/\", latest_checkpoint)\n",
    "    print(f\"\\n Model saved at: {checkpoint_path}\")\n",
    "    print(f\"File size: {os.path.getsize(checkpoint_path) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(\" No checkpoints found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:40:05.040359Z",
     "iopub.status.busy": "2025-12-25T03:40:05.039574Z",
     "iopub.status.idle": "2025-12-25T03:40:05.368059Z",
     "shell.execute_reply": "2025-12-25T03:40:05.367104Z",
     "shell.execute_reply.started": "2025-12-25T03:40:05.040332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from model import SpeechRecognition\n",
    "from dataset import get_featurizer\n",
    "from utils import TextProcess\n",
    "import json\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model for inference...\")\n",
    "h_params = SpeechRecognition.hyper_parameters\n",
    "model = SpeechRecognition(**h_params)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Load sample\n",
    "with open(\"valid_data.json\", 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "\n",
    "print(f\"\\n Testing on: {sample['key']}\")\n",
    "print(f\" True text: {sample['text']}\")\n",
    "\n",
    "# Predict\n",
    "waveform, sr = torchaudio.load(sample['key'])\n",
    "if sr != 8000:\n",
    "    resampler = torchaudio.transforms.Resample(sr, 8000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "featurizer = get_featurizer(8000)\n",
    "log_mel = featurizer(waveform).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden = model._init_hidden(1)\n",
    "    output, _ = model(log_mel, hidden)\n",
    "    output = torch.nn.functional.softmax(output, dim=2)\n",
    "    \n",
    "    # Greedy decode\n",
    "    arg_maxes = torch.argmax(output, dim=2).squeeze()\n",
    "    decode = []\n",
    "    for i, index in enumerate(arg_maxes):\n",
    "        if index != 28:  # blank\n",
    "            if i == 0 or index != arg_maxes[i-1]:\n",
    "                decode.append(index.item())\n",
    "    \n",
    "    text_process = TextProcess()\n",
    "    predicted_text = text_process.int_to_text_sequence(decode)\n",
    "    \n",
    "    print(f\" Predicted: {predicted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T03:40:09.653350Z",
     "iopub.status.busy": "2025-12-25T03:40:09.652588Z",
     "iopub.status.idle": "2025-12-25T03:40:09.659294Z",
     "shell.execute_reply": "2025-12-25T03:40:09.658378Z",
     "shell.execute_reply.started": "2025-12-25T03:40:09.653321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy best model to working directory for easy download\n",
    "output_path = \"/kaggle/working/speech_recognition_model.ckpt\"\n",
    "shutil.copy(checkpoint_path, output_path)\n",
    "\n",
    "print(f\" Model saved to: {output_path}\")\n",
    "print(f\" Download from 'Output' tab in Kaggle\")\n",
    "print(f\"\\nTo use this model:\")\n",
    "print(\"1. Download the .ckpt file\")\n",
    "print(\"2. Load with PyTorch\")\n",
    "print(\"3. Use for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\" TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n Model checkpoint: {checkpoint_path}\")\n",
    "print(f\" Download ready: /kaggle/working/speech_recognition_model.ckpt\")\n",
    "print(f\"\\n Training Stats:\")\n",
    "print(f\"  â€¢ Training samples: {train_count}\")\n",
    "print(f\"  â€¢ Validation samples: {valid_count}\")\n",
    "print(f\"  â€¢ Epochs: 10\")\n",
    "print(f\"  â€¢ Batch size: 16\")\n",
    "print(f\"\\n Next Steps:\")\n",
    "print(\"  1. Download model from Output tab\")\n",
    "print(\"  2. Test on your own audio\")\n",
    "print(\"  3. Fine-tune with more epochs if needed\")\n",
    "print(\"  4. Try larger dataset for better accuracy\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3868867,
     "sourceId": 6713857,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
